Text
classification
</s>
</s>
Text
classification
is
a
core
problem
to
many
applications,
like
spam
detection,
sentiment
analysis
or
smart
replies.
In
this
tutorial,
we
describe
how
to
build
a
text
classifier
with
the
fastText
tool.
</s>
What
is
text
classification?
#
</s>
</s>
The
goal
of
text
classification
is
to
assign
documents
(such
as
emails,
posts,
text
messages,
product
reviews,
etc...)
to
one
or
multiple
categories.
Such
categories
can
be
review
scores,
spam
v.s.
non-spam,
or
the
language
in
which
the
document
was
typed.
Nowadays,
the
dominant
approach
to
build
such
classifiers
is
machine
learning,
that
is
learning
classification
rules
from
examples.
In
order
to
build
such
classifiers,
we
need
labeled
data,
which
consists
of
documents
and
their
corresponding
categories
(or
tags,
or
labels).
</s>
</s>
As
an
example,
we
build
a
classifier
which
automatically
classifies
stackexchange
questions
about
cooking
into
one
of
several
possible
tags,
such
as
pot,
bowl
or
baking.
</s>
Installing
fastText
#
</s>
</s>
The
first
step
of
this
tutorial
is
to
install
and
build
fastText.
It
only
requires
a
c++
compiler
with
good
support
of
c++11.
</s>
</s>
Let
us
start
by
cloning
the
fastText
repository:
</s>
</s>
>>
git
clone
git@github.com/facebookresearch/fastText.git
</s>
</s>
Move
to
the
fastText
directory
and
build
it:
</s>
</s>
>>
cd
fastText
&&
make
</s>
</s>
Running
the
binary
without
any
argument
will
print
the
high
level
documentation,
showing
the
different
usecases
supported
by
fastText:
</s>
</s>
>>
./fasttext
</s>
usage:
fasttext
<command>
<args>
</s>
</s>
The
commands
supported
by
fasttext
are:
</s>
</s>
supervised
train
a
supervised
classifier
</s>
test
evaluate
a
supervised
classifier
</s>
predict
predict
most
likely
labels
</s>
predict-prob
predict
most
likely
labels
with
probabilities
</s>
skipgram
train
a
skipgram
model
</s>
cbow
train
a
cbow
model
</s>
print-vectors
print
vectors
given
a
trained
model
</s>
</s>
In
this
tutorial,
we
mainly
use
the
supervised,
test
and
predict
subcommands,
which
corresponds
to
learning
(and
using)
text
classifier.
For
an
introduction
to
the
other
functionalities
of
fastText,
please
see
the
tutorial
about
learning
word
vectors.
</s>
Getting
and
preparing
the
data
#
</s>
</s>
As
mentioned
in
the
introduction,
we
need
labeled
data
to
train
our
supervised
classifier.
In
this
tutorial,
we
are
interested
in
building
a
classifier
to
automatically
recognize
the
topic
of
a
stackexchange
question
about
cooking.
Let's
download
examples
of
questions
from
the
cooking
section
of
Stackexchange,
and
their
associated
tags:
</s>
</s>
>>
wget
https://s3-us-west-1.amazonaws.com/fasttext-vectors/cooking.stackexchange.tar.gz
&&
tar
xvzf
cooking.stackexchange.tar.gz
</s>
>>
head
cooking.stackexchange.txt
</s>
</s>
Each
line
of
the
text
file
contains
a
list
of
labels,
followed
by
the
corresponding
document.
All
the
labels
start
by
the
__label__
prefix,
which
is
how
fastText
recognize
what
is
a
label
or
what
is
a
word.
The
model
is
then
trained
to
predict
the
labels
given
the
word
in
the
document.
</s>
</s>
Before
training
our
first
classifier,
we
need
to
split
the
data
into
train
and
validation.
We
will
use
the
validation
set
to
evaluate
how
good
the
learned
classifier
is
on
new
data.
</s>
</s>
>>
wc
cooking.stackexchange.txt
</s>
15404
169582
1401900
cooking.stackexchange.txt
</s>
</s>
Our
full
dataset
contains
15404
examples.
Let's
split
it
into
a
training
set
of
12404
examples
and
a
validation
set
of
3000
examples:
</s>
</s>
>>
head
-n
12404
cooking.stackexchange.txt
>
cooking.train
</s>
>>
tail
-n
3000
cooking.stackexchange.txt
>
cooking.valid
</s>
</s>
Our
first
classifier
#
</s>
</s>
We
are
now
ready
to
train
our
first
classifier:
</s>
</s>
>>
./fasttext
supervised
-input
cooking.train
-output
model_cooking
</s>
Read
0M
words
</s>
Number
of
words:
14598
</s>
Number
of
labels:
734
</s>
Progress:
100.0%
words/sec/thread:
75109
lr:
0.000000
loss:
5.708354
eta:
0h0m
</s>
</s>
The
-input
command
line
option
indicates
the
file
containing
the
training
examples,
while
the
-output
option
indicates
where
to
save
the
model.
At
the
end
of
training,
a
file
model_cooking.bin,
containing
the
trained
classifier,
is
created
in
the
current
directory.
</s>
</s>
It
is
possible
to
directly
test
our
classifier
interactively,
by
running
the
command:
</s>
</s>
>>
./fasttext
predict
model_cooking.bin
-
</s>
</s>
and
then
typing
a
sentence.
Let's
first
try
the
sentence:
</s>
</s>
Which
baking
dish
is
best
to
bake
a
banana
bread
?
</s>
</s>
The
predicted
tag
is
baking
which
fits
well
to
this
question.
Let
us
now
try
a
second
example:
</s>
</s>
Why
not
put
knives
in
the
dishwasher?
</s>
</s>
The
label
predicted
by
the
model
is
food-safety,
which
is
not
relevant.
Somehow,
the
model
seems
to
fail
on
simple
examples.
To
get
a
better
sense
of
its
quality,
let's
test
it
on
the
validation
data
by
running:
</s>
</s>
>>
./fasttext
test
model_cooking.bin
cooking.valid
</s>
N
3000
</s>
P@1
0.124
</s>
R@1
0.0541
</s>
Number
of
examples:
3000
</s>
</s>
The
output
of
fastText
are
the
precision
at
one
(P@1)
and
the
recall
at
one
(R@1).
We
can
also
compute
the
precision
at
five
and
recall
at
five
with:
</s>
</s>
>>
./fasttext
test
model_cooking.bin
cooking.valid
5
</s>
N
3000
</s>
P@5
0.0668
</s>
R@5
0.146
</s>
Number
of
examples:
3000
</s>
</s>
Advanced
reader:
precision
and
recall
#
</s>
</s>
The
precision
is
the
number
of
correct
labels
among
the
labels
predicted
by
fastText.
The
recall
is
the
number
of
labels
that
successfully
were
predicted,
among
all
the
real
labels.
Let's
take
an
example
to
make
this
more
clear:
</s>
</s>
Why
not
put
knives
in
the
dishwasher?
</s>
</s>
On
Stack
Exchange,
this
sentence
is
labeled
with
three
tags:
equipment,
cleaning
and
knives.
The
top
five
labels
predicted
by
the
model
can
be
obtained
with:
</s>
</s>
>>
./fasttext
predict
model_cooking.bin
-
5
</s>
</s>
are
food-safety,
baking,
equipment,
substitutions
and
bread.
</s>
</s>
Thus,
one
out
of
five
labels
predicted
by
the
model
is
correct,
giving
a
precision
of
0.20.
Out
of
the
three
real
labels,
only
one
is
predicted
by
the
model,
giving
a
recall
of
0.33.
</s>
</s>
For
more
details,
see
the
related
Wikipedia
page.
</s>
Making
the
model
better
#
</s>
</s>
The
model
obtained
by
running
fastText
with
the
default
arguments
is
pretty
bad
at
classifying
new
questions.
Let's
try
to
improve
the
performance,
by
changing
the
default
parameters.
</s>
preprocessing
the
data
#
</s>
</s>
Looking
at
the
data,
we
observe
that
some
words
contain
uppercase
letter
or
punctuation.
One
of
the
first
step
to
improve
the
performance
of
our
model
is
to
apply
some
simple
pre-processing.
A
crude
normalization
can
be
obtained
using
command
line
tools
such
as
sed
and
tr:
</s>
</s>
>>
cat
cooking.stackexchange.txt
|
sed
-e
"s/([.!?,'/()])/
1
/g"
|
tr
"[:upper:]"
"[:lower:]"
>
cooking.preprocessed.txt
</s>
>>
head
-n
12404
cooking.preprocessed.txt
>
cooking.train
</s>
>>
tail
-n
3000
cooking.preprocessed.txt
>
cooking.valid
</s>
</s>
Let's
train
a
new
model
on
the
pre-processed
data:
</s>
</s>
>>
./fasttext
supervised
-input
cooking.train
-output
model_cooking
</s>
Read
0M
words
</s>
Number
of
words:
9012
</s>
Number
of
labels:
734
</s>
Progress:
100.0%
words/sec/thread:
82041
lr:
0.000000
loss:
5.671649
eta:
0h0m
h-14m
</s>
</s>
>>
./fasttext
test
model_cooking.bin
cooking.valid
</s>
N
3000
</s>
P@1
0.164
</s>
R@1
0.0717
</s>
Number
of
examples:
3000
</s>
</s>
We
observe
that
thanks
to
the
pre-processing,
the
vocabulary
is
smaller
(from
14k
words
to
9k).
The
precision
is
also
starting
to
go
up
by
4%!
</s>
more
epochs
and
larger
learning
rate
#
</s>
</s>
By
default,
fastText
sees
each
training
example
only
five
times
during
training,
which
is
pretty
small,
given
that
our
training
set
only
have
12k
training
examples.
The
number
of
times
each
examples
is
seen
(also
known
as
the
number
of
epochs),
can
be
increased
using
the
-epoch
option:
</s>
</s>
>>
./fasttext
supervised
-input
cooking.train
-output
model_cooking
-epoch
25
</s>
Read
0M
words
</s>
Number
of
words:
9012
</s>
Number
of
labels:
734
</s>
Progress:
100.0%
words/sec/thread:
77633
lr:
0.000000
loss:
7.147976
eta:
0h0m
</s>
</s>
Let's
test
the
new
model:
</s>
</s>
>>
./fasttext
test
model_cooking.bin
cooking.valid
</s>
N
3000
</s>
P@1
0.501
</s>
R@1
0.218
</s>
Number
of
examples:
3000
</s>
</s>
This
is
much
better!
Another
way
to
change
the
learning
speed
of
our
model
is
to
increase
(or
decrease)
the
learning
rate
of
the
algorithm.
This
corresponds
to
how
much
the
model
changes
after
processing
each
example.
A
learning
rate
of
0
would
means
that
the
model
does
not
change
at
all,
and
thus,
does
not
learn
anything.
Good
values
of
the
learning
rate
are
in
the
range
0.1
-
1.0.
</s>
</s>
>>
./fasttext
supervised
-input
cooking.train
-output
model_cooking
-lr
1.0
</s>
Read
0M
words
</s>
Number
of
words:
9012
</s>
Number
of
labels:
734
</s>
Progress:
100.0%
words/sec/thread:
81469
lr:
0.000000
loss:
6.405640
eta:
0h0m
</s>
</s>
>>
./fasttext
test
model_cooking.bin
cooking.valid
</s>
N
3000
</s>
P@1
0.563
</s>
R@1
0.245
</s>
Number
of
examples:
3000
</s>
</s>
Even
better!
Let's
try
both
together:
</s>
</s>
>>
./fasttext
supervised
-input
cooking.train
-output
model_cooking
-lr
1.0
-epoch
25
</s>
Read
0M
words
</s>
Number
of
words:
9012
</s>
Number
of
labels:
734
</s>
Progress:
100.0%
words/sec/thread:
76394
lr:
0.000000
loss:
4.350277
eta:
0h0m
</s>
</s>
>>
./fasttext
test
model_cooking.bin
cooking.valid
</s>
N
3000
</s>
P@1
0.585
</s>
R@1
0.255
</s>
Number
of
examples:
3000
</s>
</s>
Let
us
now
add
a
few
more
features
to
improve
even
further
our
performance!
</s>
word
n-grams
#
</s>
</s>
Finally,
we
can
improve
the
performance
of
a
model
by
using
word
bigrams,
instead
of
just
unigrams.
This
is
especially
important
for
classification
problems
where
word
order
is
important,
such
as
sentiment
analysis.
</s>
</s>
>>
./fasttext
supervised
-input
cooking.train
-output
model_cooking
-lr
1.0
-epoch
25
-wordNgrams
2
</s>
Read
0M
words
</s>
Number
of
words:
9012
</s>
Number
of
labels:
734
</s>
Progress:
100.0%
words/sec/thread:
75366
lr:
0.000000
loss:
3.226064
eta:
0h0m
</s>
</s>
>>
./fasttext
test
model_cooking.bin
cooking.valid
</s>
N
3000
</s>
P@1
0.599
</s>
R@1
0.261
</s>
Number
of
examples:
3000
</s>
</s>
With
a
few
steps,
we
were
able
to
go
from
a
precision
at
one
of
12.4%
to
59.9%.
Important
steps
included:
</s>
</s>
preprocessing
the
data
;
</s>
changing
the
number
of
epochs
(using
the
option
-epoch,
standard
range
[5
-
50])
;
</s>
changing
the
learning
rate
(using
the
option
-lr,
standard
range
[0.1
-
1.0])
;
</s>
using
word
n-grams
(using
the
option
-wordNgrams,
standard
range
[1
-
5]).
</s>
</s>
Advanced
readers:
What
is
a
Bigram?
#
</s>
</s>
A
'unigram'
refers
to
a
single
undividing
unit,
or
token,
usually
used
as
an
input
to
a
model.
For
example
a
unigram
can
a
word
or
a
letter
depending
on
the
model.
In
fastText,
we
work
at
the
word
level
and
thus
unigrams
are
words.
</s>
</s>
Similarly
we
denote
by
'bigram'
the
concatenation
of
2
consecutive
tokens
or
words.
Similarly
we
often
talk
about
n-gram
to
refer
to
the
concatenation
any
n
consecutive
tokens.
</s>
</s>
For
example,
in
the
sentence,
'Last
donut
of
the
night',
the
unigrams
are
'last',
'donut',
'of',
'the'
and
'night'.
The
bigrams
are:
'Last
donut',
'donut
of',
'of
the'
and
'the
night'.
</s>
</s>
Bigrams
are
particularly
interesting
because,
for
most
sentences,
you
can
reconstruct
the
order
of
the
words
just
by
looking
at
a
bag
of
n-grams.
</s>
</s>
Let
us
illustrate
this
by
a
simple
exercise,
given
the
following
bigrams,
try
to
reconstruct
the
original
sentence:
'all
out',
'I
am',
'of
bubblegum',
'out
of'
and
'am
all'.
It
is
common
to
refer
to
a
word
as
a
unigram.
</s>
Scaling
things
up
#
</s>
</s>
Since
we
are
training
our
model
on
a
few
thousands
of
examples,
the
training
only
takes
a
few
seconds.
But
training
models
on
larger
datasets,
with
more
labels
can
start
to
be
too
slow.
A
potential
solution
to
make
the
training
faster
is
to
use
the
hierarchical
softmax,
instead
of
the
regular
softmax
[Add
a
quick
explanation
of
the
hierarchical
softmax].
This
can
be
done
with
the
option
-loss
hs:
</s>
</s>
>>
./fasttext
supervised
-input
cooking.train
-output
model_cooking
-lr
1.0
-epoch
25
-wordNgrams
2
-bucket
200000
-dim
50
-loss
hs
</s>
Read
0M
words
</s>
Number
of
words:
9012
</s>
Number
of
labels:
734
</s>
Progress:
100.0%
words/sec/thread:
2199406
lr:
0.000000
loss:
1.718807
eta:
0h0m
</s>
</s>
Training
should
now
take
less
than
a
second.
</s>
Conclusion
#
</s>
</s>
In
this
tutorial,
we
gave
a
brief
overview
of
how
to
use
fastText
to
train
powerful
text
classifiers.
We
had
a
light
overview
of
some
of
the
most
important
options
to
tune.
</s>
</s>
Word
representations
</s>
</s>
A
popular
idea
in
modern
machine
learning
is
to
represent
words
by
vectors.
These
vectors
capture
hidden
information
about
a
language,
like
word
analogies
or
semantic.
It
is
also
used
to
improve
performance
of
text
classifiers.
</s>
</s>
In
this
tutorial,
we
show
how
to
build
these
word
vectors
with
the
fastText
tool.
To
download
and
install
fastText,
follow
the
first
steps
of
the
tutorial
on
text
classification.
</s>
Getting
the
data
#
</s>
</s>
In
order
to
compute
word
vectors,
you
need
a
large
text
corpus.
Depending
on
the
corpus,
the
word
vectors
will
capture
different
information.
In
this
tutorial,
we
focus
on
Wikipedia's
articles
but
other
sources
could
be
considered,
like
news
or
Webcrawl
(more
examples
here).
To
download
a
raw
dump
of
Wikipedia,
run
the
following
command:
</s>
</s>
wget
https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
</s>
</s>
Downloading
the
Wikipedia
corpus
takes
some
time.
Instead,
lets
restrict
our
study
to
the
first
1
billion
bytes
of
English
Wikipedia.
They
can
be
found
on
Matt
Mahoney's
website:
</s>
</s>
$
mkdir
data
</s>
$
wget
-c
http://mattmahoney.net/dc/enwik9.zip
-P
data
</s>
$
unzip
data/enwik9.zip
-d
data
</s>
</s>
A
raw
Wikipedia
dump
contains
a
lot
of
HTML
/
XML
data.
We
pre-process
it
with
the
wikifil.pl
script
bundled
with
fastText
(this
script
was
originally
developed
by
Matt
Mahoney,
and
can
be
found
on
his
website
)
</s>
</s>
$
perl
wikifil.pl
data/enwik9
>
data/fil9
</s>
</s>
We
can
check
the
file
by
running
the
following
command:
</s>
</s>
$
head
-c
80
data/text9
</s>
anarchism
originated
as
a
term
of
abuse
first
used
against
early
working
class
</s>
</s>
The
text
is
nicely
pre-processed
and
can
be
used
to
learn
our
word
vectors.
</s>
Training
word
vectors
#
</s>
</s>
Learning
word
vectors
on
this
data
can
now
be
achieved
with
a
single
command:
</s>
</s>
$
mkdir
result
</s>
$
./fasttext
skipgram
-input
data/fil9
-output
result/fil9
</s>
</s>
To
decompose
this
command
line:
./fastext
calls
the
binary
fastText
executable
(see
how
to
install
fastText
here)
with
the
'skipgram'
model
(it
can
also
be
'cbow').
We
then
specify
the
requires
options
'-input'
for
the
location
of
the
data
and
'-output'
for
the
location
where
the
word
representations
will
be
saved.
</s>
</s>
While
fastText
is
running,
the
progress
and
estimated
time
to
completion
is
shown
on
your
screen.
Once
the
program
finishes,
there
should
be
two
files
in
the
result
directory:
</s>
</s>
$
ls
-l
result
</s>
-rw-r-r--
1
bojanowski
1876110778
978480850
Dec
20
11:01
fil9.bin
</s>
-rw-r-r--
1
bojanowski
1876110778
190004182
Dec
20
11:01
fil9.vec
</s>
</s>
The
fil9.bin
file
is
a
binary
file
that
stores
the
whole
fastText
model
and
can
be
subsequently
loaded.
The
fil9.vec
file
is
a
text
file
that
contains
the
word
vectors,
one
per
line
for
each
word
in
the
vocabulary:
</s>
</s>
$
head
-n
4
result/fil9.vec
</s>
218316
100
</s>
the
-0.10363
-0.063669
0.032436
-0.040798
0.53749
0.00097867
0.10083
0.24829
...
</s>
of
-0.0083724
0.0059414
-0.046618
-0.072735
0.83007
0.038895
-0.13634
0.60063
...
</s>
one
0.32731
0.044409
-0.46484
0.14716
0.7431
0.24684
-0.11301
0.51721
0.73262
...
</s>
</s>
The
first
line
is
a
header
containing
the
number
of
words
and
the
dimensionality
of
the
vectors.
The
subsequent
lines
are
the
word
vectors
for
all
words
in
the
vocabulary,
sorted
by
decreasing
frequency.
</s>
Advanced
readers:
skipgram
versus
cbow
#
</s>
</s>
fastText
provides
two
models
for
computing
word
representations:
skipgram
and
cbow
('continuous-bag-of-words').
</s>
</s>
The
skipgram
model
learns
to
predict
a
target
word
thanks
to
a
nearby
word.
On
the
other
hand,
the
cbow
model
predicts
the
target
word
according
to
its
context.
The
context
is
represented
as
a
bag
of
the
words
contained
in
a
fixed
size
window
around
the
target
word.
</s>
</s>
Let
us
illustrate
this
difference
with
an
example:
given
the
sentence
'Poets
have
been
mysteriously
silent
on
the
subject
of
cheese'
and
the
target
word
'silent',
a
skipgram
model
tries
to
predict
the
target
using
a
random
close-by
word,
like
'subject'
or
'mysteriously'**.
The
cbow
model
takes
all
the
words
in
a
surrounding
window,
like
{been,
mysteriously,
on,
the},
and
uses
the
sum
of
their
vectors
to
predict
the
target.
The
figure
below
summarizes
this
difference
with
another
example.
</s>
</s>
cbow
vs
skipgram
To
train
a
cbow
model
with
fastText,
you
run
the
following
command:
</s>
</s>
./fasttext
cbow
-input
data/fil9
-output
result/fil9
</s>
</s>
In
practice,
we
observe
that
skipgram
models
works
better
with
subword
information
than
cbow.
</s>
Advanced
readers:
playing
with
the
parameters
#
</s>
</s>
So
far,
we
run
fastText
with
the
default
parameters,
but
depending
on
the
data,
these
parameters
may
not
be
optimal.
Let
us
give
an
introduction
to
some
of
the
key
parameters
for
word
vectors.
</s>
</s>
The
most
important
parameters
of
the
model
are
its
dimension
and
the
range
of
size
for
the
subwords.
The
dimension
(dim)
controls
the
size
of
the
vectors,
the
larger
they
are
the
more
information
they
can
capture
but
requires
more
data
to
be
learned.
But,
if
they
are
too
large,
they
are
harder
and
slower
to
train.
By
default,
we
use
100
dimensions,
but
any
value
in
the
100-300
range
is
as
popular.
The
subwords
are
all
the
substrings
contained
in
a
word
between
the
minimum
size
(nmin)
and
the
maximal
size
(nmax).
By
default,
we
take
all
the
subword
between
3
and
6
characters,
but
other
range
could
be
more
appropriate
to
different
languages:
</s>
</s>
$
./fasttext
skipgram
-input
data/fil9
-output
result/fil9
-nmin
2
-nmax
5
-dim
300
</s>
</s>
Depending
on
the
quantity
of
data
you
have,
you
may
want
to
change
the
parameters
of
the
training.
The
epoch
parameter
controls
how
many
time
will
loop
over
your
data.
By
default,
we
loop
over
the
dataset
5
times.
If
you
dataset
is
extremely
massive,
you
may
want
to
loop
over
it
less
often.
Another
important
parameter
is
the
learning
rate
-lr).
The
higher
the
learning
rate
is,
the
faster
the
model
converge
to
a
solution
but
at
the
risk
of
overfitting
to
the
dataset.
The
default
value
is
0.05
which
is
a
good
compromise.
If
you
want
to
play
with
it
we
suggest
to
stay
in
the
range
of
[0.01,
1]:
</s>
</s>
$
./fasttext
skipgram
-input
data/fil9
-output
result/fil9
-epoch
1
-lr
0.5
</s>
</s>
Finally
,
fastText
is
multi-threaded
and
uses
12
threads
by
default.
If
you
have
less
CPU
cores
(say
4),
you
can
easily
set
the
number
of
threads
using
the
thread
flag:
</s>
</s>
$
./fasttext
skipgram
-input
data/fil9
-output
result/fil9
-thread
4
</s>
</s>
Printing
word
vectors
#
</s>
</s>
Searching
and
printing
word
vectors
directly
from
the
fil9.vec
file
is
cumbersome.
Fortunately,
there
is
a
print-vectors
functionality
in
fastText.
</s>
</s>
For
examples,
we
can
print
the
word
vectors
of
words
asparagus,
pidgey
and
yellow
with
the
following
command:
</s>
</s>
$
echo
"asparagus
pidgey
yellow"
|
./fasttext
print-vectors
result/fil9.bin
</s>
asparagus
0.46826
-0.20187
-0.29122
-0.17918
0.31289
-0.31679
0.17828
-0.04418
...
</s>
pidgey
-0.16065
-0.45867
0.10565
0.036952
-0.11482
0.030053
0.12115
0.39725
...
</s>
yellow
-0.39965
-0.41068
0.067086
-0.034611
0.15246
-0.12208
-0.040719
-0.30155
...
</s>
</s>
A
nice
feature
is
that
you
can
also
query
for
words
that
did
not
appear
in
your
data!
Indeed
words
are
represented
by
the
sum
of
its
substrings.
As
long
as
the
unknown
word
is
made
of
known
substrings,
there
is
a
representation
of
it!
</s>
</s>
As
an
example
let's
try
with
a
misspelled
word:
</s>
</s>
$
echo
"enviroment"
|
./fasttext
print-vectors
result/fil9.bin
</s>
</s>
You
still
get
a
word
vector
for
it!
But
how
good
it
is?
Let
s
find
out
in
the
next
sections!
</s>
Nearest
neighbor
queries
#
</s>
</s>
A
simple
way
to
check
the
quality
of
a
word
vector
is
to
look
at
its
nearest
neighbors.
This
give
an
intuition
of
the
type
of
semantic
information
the
vectors
are
able
to
capture.
</s>
</s>
This
can
be
achieve
with
the
nn
functionality.
For
example,
we
can
query
the
10
nearest
neighbors
of
a
word
by
running
the
following
command:
</s>
</s>
$
./fasttext
nn
result/fil9.bin
</s>
Pre-computing
word
vectors...
done.
</s>
</s>
Then
we
are
prompted
to
type
our
query
word,
let
us
try
asparagus
:
</s>
</s>
Query
word?
asparagus
</s>
beetroot
0.812384
</s>
tomato
0.806688
</s>
horseradish
0.805928
</s>
spinach
0.801483
</s>
licorice
0.791697
</s>
lingonberries
0.781507
</s>
asparagales
0.780756
</s>
lingonberry
0.778534
</s>
celery
0.774529
</s>
beets
0.773984
</s>
</s>
Nice!
It
seems
that
vegetable
vectors
are
similar.
Note
that
the
nearest
neighbor
is
the
word
asparagus
itself,
this
means
that
this
word
appeared
in
the
dataset.
What
about
pokemons?
</s>
</s>
Query
word?
pidgey
</s>
pidgeot
0.891801
</s>
pidgeotto
0.885109
</s>
pidge
0.884739
</s>
pidgeon
0.787351
</s>
pok
0.781068
</s>
pikachu
0.758688
</s>
charizard
0.749403
</s>
squirtle
0.742582
</s>
beedrill
0.741579
</s>
charmeleon
0.733625
</s>
</s>
Different
evolution
of
the
same
Pokemon
have
close-by
vectors!
But
what
about
our
misspelled
word,
is
its
vector
close
to
anything
reasonable?
Let
s
find
out:
</s>
</s>
Query
word?
enviroment
</s>
enviromental
0.907951
</s>
environ
0.87146
</s>
enviro
0.855381
</s>
environs
0.803349
</s>
environnement
0.772682
</s>
enviromission
0.761168
</s>
realclimate
0.716746
</s>
environment
0.702706
</s>
acclimatation
0.697196
</s>
ecotourism
0.697081
</s>
</s>
Thanks
to
the
information
contained
within
the
word,
the
vector
of
our
misspelled
word
matches
to
reasonable
words!
It
is
not
perfect
but
the
main
information
has
been
captured.
</s>
Advanced
reader:
measure
of
similarity
#
</s>
</s>
In
order
to
find
nearest
neighbors,
we
need
to
compute
a
similarity
score
between
words.
Our
words
are
represented
by
continuous
word
vectors
and
we
can
thus
apply
simple
similarities
to
them.
In
particular
we
use
the
cosine
of
the
angles
between
two
vectors.
This
similarity
is
computed
for
all
words
in
the
vocabulary,
and
the
10
most
similar
words
are
shown.
Of
course,
if
the
word
appears
in
the
vocabulary,
it
will
appear
on
top,
with
a
similarity
of
1.
</s>
Word
analogies
#
</s>
</s>
In
a
similar
spirit,
one
can
play
around
with
word
analogies.
For
example,
we
can
see
if
our
model
can
guess
what
is
to
France,
what
Berlin
is
to
Germany.
</s>
</s>
This
can
be
done
with
the
analogies
functionality.
It
takes
a
word
triplet
(like
Germany
Berlin
France)
and
outputs
the
analogy:
</s>
</s>
$
./fasttext
analogies
result/fil9.bin
</s>
Pre-computing
word
vectors...
done.
</s>
Query
triplet
(A
-
B
+
C)?
berlin
germany
france
</s>
paris
0.896462
</s>
bourges
0.768954
</s>
louveciennes
0.765569
</s>
toulouse
0.761916
</s>
valenciennes
0.760251
</s>
montpellier
0.752747
</s>
strasbourg
0.744487
</s>
meudon
0.74143
</s>
bordeaux
0.740635
</s>
pigneaux
0.736122
</s>
</s>
The
answer
provides
by
our
model
is
Paris,
which
is
correct.
Let's
have
a
look
at
a
less
obvious
example:
</s>
</s>
Query
triplet
(A
-
B
+
C)?
psx
sony
nintendo
</s>
gamecube
0.803352
</s>
nintendogs
0.792646
</s>
playstation
0.77344
</s>
sega
0.772165
</s>
gameboy
0.767959
</s>
arcade
0.754774
</s>
playstationjapan
0.753473
</s>
gba
0.752909
</s>
dreamcast
0.74907
</s>
famicom
0.745298
</s>
</s>
Our
model
considers
that
the
nintendo
analogy
of
a
psx
is
the
gamecube,
which
seems
reasonable.
Of
course
the
quality
of
the
analogies
depend
on
the
dataset
used
to
train
the
model
and
one
can
only
hope
to
cover
fields
only
in
the
dataset.
</s>
Importance
of
character
n-grams
#
</s>
</s>
Using
subword-level
information
is
particularly
interesting
to
build
vectors
for
unknown
words.
For
example,
the
word
gearshift
does
not
exist
on
Wikipedia
but
we
can
still
query
its
closest
existing
words:
</s>
</s>
Query
word?
gearshift
</s>
gearing
0.790762
</s>
flywheels
0.779804
</s>
flywheel
0.777859
</s>
gears
0.776133
</s>
driveshafts
0.756345
</s>
driveshaft
0.755679
</s>
daisywheel
0.749998
</s>
wheelsets
0.748578
</s>
epicycles
0.744268
</s>
gearboxes
0.73986
</s>
</s>
Most
of
the
retrieved
words
share
substantial
substrings
but
a
few
are
actually
quite
different,
like
cogwheel.
You
can
try
other
words
like
sunbathe
or
grandnieces.
</s>
</s>
Now
that
we
have
seen
the
interest
of
subword
information
for
unknown
words,
let
s
check
how
it
compares
to
a
model
that
do
not
use
subword
information.
To
train
a
model
without
no
subwords,
just
run
the
following
command:
</s>
</s>
$
./fasttext
skipgram
-input
data/fil9
-output
result/fil9-none
-maxn
0
</s>
</s>
The
results
are
saved
in
result/fil9-non.vec
and
result/fil9-non.bin.
</s>
</s>
To
illustrate
the
difference,
let
us
take
an
uncommon
word
in
Wikipedia,
like
accomodation
which
is
a
misspelling
of
accommodation**.
Here
is
the
nearest
neighbors
obtained
without
no
subwords:
</s>
</s>
$
./fasttext
nn
result/fil9-none.bin
</s>
Query
word?
accomodation
</s>
sunnhordland
0.775057
</s>
accomodations
0.769206
</s>
administrational
0.753011
</s>
laponian
0.752274
</s>
ammenities
0.750805
</s>
dachas
0.75026
</s>
vuosaari
0.74172
</s>
hostelling
0.739995
</s>
greenbelts
0.733975
</s>
asserbo
0.732465
</s>
</s>
The
result
does
not
make
much
sense,
most
of
these
words
are
unrelated.
On
the
other
hand,
using
subword
information
gives
the
following
list
of
nearest
neighbors:
</s>
</s>
Query
word?
accomodation
</s>
accomodations
0.96342
</s>
accommodation
0.942124
</s>
accommodations
0.915427
</s>
accommodative
0.847751
</s>
accommodating
0.794353
</s>
accomodated
0.740381
</s>
amenities
0.729746
</s>
catering
0.725975
</s>
accomodate
0.703177
</s>
hospitality
0.701426
</s>
</s>
The
nearest
neighbors
capture
different
variation
around
the
word
accommodation.
We
also
get
semantically
related
words
such
as
amenities
or
lodging.
</s>
Conclusion
#
</s>
</s>
In
this
tutorial,
we
show
how
to
obtain
word
vectors
from
Wikipedia.
This
can
be
done
for
any
language
and
you
can
find
pre-trained
models
with
the
default
setting
for
294
of
them
here
</s>
</s>
FAQ
</s>
What
is
fastText?
Are
there
tutorials?
#
</s>
</s>
FastText
is
a
library
for
text
classification
and
representation.
It
transforms
text
into
continuous
vectors
that
can
later
be
used
on
any
language
related
task.
A
few
tutorials
are
available.
</s>
Why
are
my
fastText
models
that
big?
#
</s>
</s>
fastText
uses
a
hashtable
for
either
word
or
character
ngrams.
The
size
of
the
hashtable
direclty
impacts
the
size
of
a
model.
To
reduce
the
size
of
the
model,
it
is
possible
to
reduce
the
size
of
this
table
with
the
option
'-hash'.
For
example
a
good
value
is
20000.
Another
option
that
greatly
impacts
the
size
of
a
model
is
the
size
of
the
vectors
(-dim).
This
dimension
can
be
reduced
to
save
space
but
this
can
significantly
impact
performance.
If
that
still
produce
a
model
that
is
too
big,
one
can
further
reduce
the
size
of
a
trained
model
with
the
quantization
option.
</s>
</s>
./fasttext
quantize
-output
model
</s>
</s>
What
would
be
the
best
way
to
represent
word
phrases
rather
than
words?
#
</s>
</s>
Currently
the
best
approach
to
represent
word
phrases
or
sentence
is
to
take
a
bag
of
words
of
word
vectors.
Additionally,
for
phrases
like
“New
York”,
preprocessing
the
data
so
that
it
becomes
a
single
token
“New_York”
can
greatly
help.
</s>
Why
does
fastText
produce
vectors
even
for
unknown
words?
#
</s>
</s>
One
of
the
key
features
of
fastText
word
representation
is
its
ability
to
produce
vectors
for
any
words,
even
made-up
ones.
Indeed,
fastText
word
vectors
are
built
from
vectors
of
substrings
of
characters
contained
in
it.
This
allows
to
build
vectors
even
for
misspelled
words
or
concatenation
of
words.
</s>
Why
is
the
hierarchical
softmax
slightly
worse
in
performance
than
the
full
softmax?
#
</s>
</s>
The
hierachical
softmax
is
an
approximation
of
the
full
softmax
loss
that
allows
to
train
on
large
number
of
class
efficiently.
This
is
often
at
the
cost
of
a
few
percent
of
accuracy.
Note
also
that
this
loss
is
thought
for
classes
that
are
unbalanced,
that
is
some
classes
are
more
frequent
than
others.
If
your
dataset
has
a
balanced
number
of
examples
per
class,
it
is
worth
trying
the
negative
sampling
loss
(-loss
ns
-neg
100).
However,
negative
sampling
will
still
be
very
slow
at
test
time,
since
the
full
softmax
will
be
computed.
</s>
Can
we
run
fastText
program
on
a
GPU?
#
</s>
</s>
FastText
only
works
on
CPU
for
accessibility.
That
being
said,
fastText
has
been
implemented
in
the
caffe2
library
which
can
be
run
on
GPU.
</s>
Can
I
use
fastText
with
python?
Or
other
languages?
#
</s>
</s>
There
are
few
unofficial
wrappers
for
python
or
lua
available
on
github.
</s>
Can
I
use
fastText
with
continuous
data?
#
</s>
</s>
FastText
works
on
discrete
tokens
and
thus
cannot
be
directly
used
on
continuous
tokens.
However,
one
can
discretize
continuous
tokens
to
use
fastText
on
them,
for
example
by
rounding
values
to
a
specific
digit
("12.3"
becomes
"12").
</s>
There
are
misspellings
in
the
dictionary.
Should
we
improve
text
normalization?
#
</s>
</s>
If
the
words
are
infrequent,
there
is
no
need
to
worry.
</s>
My
compiler
/
architecture
can't
build
fastText.
What
should
I
do?
#
</s>
</s>
Try
a
newer
version
of
your
compiler.
We
try
to
maintain
compatibility
with
older
versions
of
gcc
and
many
platforms,
however
sometimes
maintaining
backwards
compatibility
becomes
very
hard.
In
general,
compilers
and
tool
chains
that
ship
with
LTS
versions
of
major
linux
distributions
should
be
fair
game.
In
any
case,
create
an
issue
with
your
compiler
version
and
architecture
and
we'll
try
to
implement
compatibility.
</s>
</s>
References
</s>
</s>
Please
cite
1
if
using
this
code
for
learning
word
representations
or
2
if
using
for
text
classification.
</s>
</s>
[1]
P.
Bojanowski*,
E.
Grave*,
A.
Joulin,
T.
Mikolov,
Enriching
Word
Vectors
with
Subword
Information
</s>
</s>
@article{bojanowski2016enriching,
</s>
title={Enriching
Word
Vectors
with
Subword
Information},
</s>
author={Bojanowski,
Piotr
and
Grave,
Edouard
and
Joulin,
Armand
and
Mikolov,
Tomas},
</s>
journal={arXiv
preprint
arXiv:1607.04606},
</s>
year={2016}
</s>
}
</s>
</s>
[2]
A.
Joulin,
E.
Grave,
P.
Bojanowski,
T.
Mikolov,
Bag
of
Tricks
for
Efficient
Text
Classification
</s>
</s>
@article{joulin2016bag,
</s>
title={Bag
of
Tricks
for
Efficient
Text
Classification},
</s>
author={Joulin,
Armand
and
Grave,
Edouard
and
Bojanowski,
Piotr
and
Mikolov,
Tomas},
</s>
journal={arXiv
preprint
arXiv:1607.01759},
</s>
year={2016}
</s>
}
</s>
</s>
[3]
A.
Joulin,
E.
Grave,
P.
Bojanowski,
M.
Douze,
H.
Jégou,
T.
Mikolov,
FastText.zip:
Compressing
text
classification
models
</s>
</s>
@article{joulin2016fasttext,
</s>
title={FastText.zip:
Compressing
text
classification
models},
</s>
author={Joulin,
Armand
and
Grave,
Edouard
and
Bojanowski,
Piotr
and
Douze,
Matthijs
and
J{\'e}gou,
H{\'e}rve
and
Mikolov,
Tomas},
</s>
journal={arXiv
preprint
arXiv:1612.03651},
</s>
year={2016}
</s>
}
</s>
</s>
(*
These
authors
contributed
equally.)
</s>
List
of
options
</s>
</s>
Invoke
a
command
without
arguments
to
list
available
arguments
and
their
default
values:
</s>
</s>
$
./fasttext
supervised
</s>
Empty
input
or
output
path.
</s>
</s>
The
following
arguments
are
mandatory:
</s>
-input
training
file
path
</s>
-output
output
file
path
</s>
</s>
The
following
arguments
are
optional:
</s>
-verbose
verbosity
level
[2]
</s>
</s>
The
following
arguments
for
the
dictionary
are
optional:
</s>
-minCount
minimal
number
of
word
occurences
[5]
</s>
-minCountLabel
minimal
number
of
label
occurences
[0]
</s>
-wordNgrams
max
length
of
word
ngram
[1]
</s>
-bucket
number
of
buckets
[2000000]
</s>
-minn
min
length
of
char
ngram
[3]
</s>
-maxn
max
length
of
char
ngram
[6]
</s>
-t
sampling
threshold
[0.0001]
</s>
-label
labels
prefix
[__label__]
</s>
</s>
The
following
arguments
for
training
are
optional:
</s>
-lr
learning
rate
[0.05]
</s>
-lrUpdateRate
change
the
rate
of
updates
for
the
learning
rate
[100]
</s>
-dim
size
of
word
vectors
[100]
</s>
-ws
size
of
the
context
window
[5]
</s>
-epoch
number
of
epochs
[5]
</s>
-neg
number
of
negatives
sampled
[5]
</s>
-loss
loss
function
{ns,
hs,
softmax}
[ns]
</s>
-thread
number
of
threads
[12]
</s>
-pretrainedVectors
pretrained
word
vectors
for
supervised
learning
[]
</s>
-saveOutput
whether
output
params
should
be
saved
[0]
</s>
</s>
The
following
arguments
for
quantization
are
optional:
</s>
-cutoff
number
of
words
and
ngrams
to
retain
[0]
</s>
-retrain
finetune
embeddings
if
a
cutoff
is
applied
[0]
</s>
-qnorm
quantizing
the
norm
separately
[0]
</s>
-qout
quantizing
the
classifier
[0]
</s>
-dsub
size
of
each
sub-vector
[2]
</s>
</s>
Defaults
may
vary
by
mode.
(Word-representation
modes
skipgram
and
cbow
use
a
default
-minCount
of
5.)
</s>
</s>
List
of
options
</s>
</s>
Invoke
a
command
without
arguments
to
list
available
arguments
and
their
default
values:
</s>
</s>
$
./fasttext
supervised
</s>
Empty
input
or
output
path.
</s>
</s>
The
following
arguments
are
mandatory:
</s>
-input
training
file
path
</s>
-output
output
file
path
</s>
</s>
The
following
arguments
are
optional:
</s>
-verbose
verbosity
level
[2]
</s>
</s>
The
following
arguments
for
the
dictionary
are
optional:
</s>
-minCount
minimal
number
of
word
occurences
[5]
</s>
-minCountLabel
minimal
number
of
label
occurences
[0]
</s>
-wordNgrams
max
length
of
word
ngram
[1]
</s>
-bucket
number
of
buckets
[2000000]
</s>
-minn
min
length
of
char
ngram
[3]
</s>
-maxn
max
length
of
char
ngram
[6]
</s>
-t
sampling
threshold
[0.0001]
</s>
-label
labels
prefix
[__label__]
</s>
</s>
The
following
arguments
for
training
are
optional:
</s>
-lr
learning
rate
[0.05]
</s>
-lrUpdateRate
change
the
rate
of
updates
for
the
learning
rate
[100]
</s>
-dim
size
of
word
vectors
[100]
</s>
-ws
size
of
the
context
window
[5]
</s>
-epoch
number
of
epochs
[5]
</s>
-neg
number
of
negatives
sampled
[5]
</s>
-loss
loss
function
{ns,
hs,
softmax}
[ns]
</s>
-thread
number
of
threads
[12]
</s>
-pretrainedVectors
pretrained
word
vectors
for
supervised
learning
[]
</s>
-saveOutput
whether
output
params
should
be
saved
[0]
</s>
</s>
The
following
arguments
for
quantization
are
optional:
</s>
-cutoff
number
of
words
and
ngrams
to
retain
[0]
</s>
-retrain
finetune
embeddings
if
a
cutoff
is
applied
[0]
</s>
-qnorm
quantizing
the
norm
separately
[0]
</s>
-qout
quantizing
the
classifier
[0]
</s>
-dsub
size
of
each
sub-vector
[2]
</s>
</s>
Defaults
may
vary
by
mode.
(Word-representation
modes
skipgram
and
cbow
use
a
default
-minCount
of
5.)
</s>
</s>
Cheatsheet
</s>
Word
representation
learning
#
</s>
</s>
In
order
to
learn
word
vectors
do:
</s>
</s>
$
./fasttext
skipgram
-input
data.txt
-output
model
</s>
</s>
Obtaining
word
vectors
#
</s>
</s>
Print
word
vectors
for
a
text
file
queries.txt
containing
words.
</s>
</s>
$
./fasttext
print-word-vectors
model.bin
<
queries.txt
</s>
</s>
Text
classification
#
</s>
</s>
In
order
to
train
a
text
classifier
do:
</s>
</s>
$
./fasttext
supervised
-input
train.txt
-output
model
</s>
</s>
Once
the
model
was
trained,
you
can
evaluate
it
by
computing
the
precision
and
recall
at
k
(P@k
and
R@k)
on
a
test
set
using:
</s>
</s>
$
./fasttext
test
model.bin
test.txt
1
</s>
</s>
In
order
to
obtain
the
k
most
likely
labels
for
a
piece
of
text,
use:
</s>
</s>
$
./fasttext
predict
model.bin
test.txt
k
</s>
</s>
In
order
to
obtain
the
k
most
likely
labels
and
their
associated
probabilities
for
a
piece
of
text,
use:
</s>
</s>
$
./fasttext
predict-prob
model.bin
test.txt
k
</s>
</s>
If
you
want
to
compute
vector
representations
of
sentences
or
paragraphs,
please
use:
</s>
</s>
$
./fasttext
print-sentence-vectors
model.bin
<
text.txt
</s>
</s>
Quantization
#
</s>
</s>
In
order
to
create
a
.ftz
file
with
a
smaller
memory
footprint
do:
</s>
</s>
$
./fasttext
quantize
-output
model
</s>
</s>
All
other
commands
such
as
test
also
work
with
this
model
</s>
</s>
$
./fasttext
test
model.ftz
test.txt
</s>
</s>
Get
started
</s>
What
is
fastText?
#
</s>
</s>
fastText
is
a
library
for
efficient
learning
of
word
representations
and
sentence
classification.
</s>
Requirements
#
</s>
</s>
fastText
builds
on
modern
Mac
OS
and
Linux
distributions.
Since
it
uses
C++11
features,
it
requires
a
compiler
with
good
C++11
support.
These
include
:
</s>
</s>
(gcc-4.6.3
or
newer)
or
(clang-3.3
or
newer)
</s>
</s>
Compilation
is
carried
out
using
a
Makefile,
so
you
will
need
to
have
a
working
make.
For
the
word-similarity
evaluation
script
you
will
need:
</s>
</s>
python
2.6
or
newer
</s>
numpy
&
scipy
</s>
</s>
Building
fastText
#
</s>
</s>
In
order
to
build
fastText,
use
the
following:
</s>
</s>
$
git
clone
https://github.com/facebookresearch/fastText.git
</s>
$
cd
fastText
</s>
$
make
</s>
</s>
This
will
produce
object
files
for
all
the
classes
as
well
as
the
main
binary
fasttext.
If
you
do
not
plan
on
using
the
default
system-wide
compiler,
update
the
two
macros
defined
at
the
beginning
of
the
Makefile
(CC
and
INCLUDES).
</s>
</s>
</s>
Language
identification
</s>
</s>
October
2,
2017
</s>
</s>
Edouard
Grave
</s>
Fast
and
accurate
language
identification
using
fastText
#
</s>
</s>
We
are
excited
to
announce
that
we
are
publishing
a
fast
and
accurate
tool
for
text-based
language
identification.
It
can
recognize
more
than
170
languages,
takes
less
than
1MB
of
memory
and
can
classify
thousands
of
documents
per
second.
It
is
based
on
fastText
library
and
is
released
here
as
open
source,
free
to
use
by
everyone.
We
are
releasing
several
versions
of
the
model,
each
optimized
for
different
memory
usage,
and
compared
them
to
the
popular
tool
langid.py.
</s>
Read
More
</s>
fastText
on
mobile
</s>
</s>
May
2,
2017
</s>
</s>
Armand
Joulin
</s>
</s>
Today,
the
Facebook
AI
Research
(FAIR)
team
released
pre-trained
vectors
in
294
languages,
accompanied
by
two
quick-start
tutorials,
to
increase
fastText’s
accessibility
to
the
large
community
of
students,
software
developers,
and
researchers
interested
in
machine
learning.
fastText’s
models
now
fit
on
smartphones
and
small
computers
like
Raspberry
Pi
devices
thanks
to
a
new
functionality
that
reduces
memory
usage.
</s>
</s>
First
open-sourced
last
summer,
fastText
was
designed
to
be
accessible
to
anyone
with
generic
hardware
like
notebooks
and
X86
cloud
instances,
or
almost
any
platform
with
enough
memory.
Smartphone
and
small
computer
support
extend
fastText’s
accessibility
to
an
even
larger
community
and
a
greater
range
of
applications.
</s>
Read
More
</s>
Releasing
fastText
</s>
</s>
August
18,
2016
</s>
</s>
Edouard
Grave
</s>
Faster,
better
text
classification!
#
</s>
</s>
Understanding
the
meaning
of
words
that
roll
off
your
tongue
as
you
talk,
or
your
fingertips
as
you
tap
out
posts
is
one
of
the
biggest
technical
challenges
facing
artificial
intelligence
researchers.
But
it
is
an
essential
need.
Automatic
text
processing
forms
a
key
part
of
the
day-to-day
interaction
with
your
computer;
it’s
a
critical
component
of
everything
from
web
search
and
content
ranking
to
spam
filtering,
and
when
it
works
well,
it’s
completely
invisible
to
you.
With
the
growing
amount
of
online
data,
there
is
a
need
for
more
flexible
tools
to
better
understand
the
content
of
very
large
datasets,
in
order
to
provide
more
accurate
classification
results.
</s>
</s>
To
address
this
need,
the
Facebook
AI
Research
(FAIR)
lab
is
open-sourcing
fastText,
a
library
designed
to
help
build
scalable
solutions
for
text
representation
and
classification.
Our
ongoing
commitment
to
collaboration
and
sharing
with
the
community
extends
beyond
just
delivering
code.
We
know
it’s
important
to
share
our
learnings
to
advance
the
field,
so
have
also
published
our
research
relating
to
fastText.
</s>
</s>
FastText
combines
some
of
the
most
successful
concepts
introduced
by
the
natural
language
processing
and
machine
learning
communities
in
the
last
few
decades.
These
include
representing
sentences
with
bag
of
words
and
bag
of
n-grams,
as
well
as
using
subword
information,
and
sharing
information
across
classes
through
a
hidden
representation.
We
also
employ
a
hierachical
softmax
that
takes
advantage
of
the
unbalanced
distribution
of
the
classes
to
speed
up
computation.
These
different
concepts
are
being
used
for
two
different
tasks:
efficient
text
classification
and
learning
word
vector
representations.
</s>
</s>
Releasing
fastText
</s>
</s>
August
18,
2016
</s>
</s>
Edouard
Grave
</s>
Faster,
better
text
classification!
#
</s>
</s>
Understanding
the
meaning
of
words
that
roll
off
your
tongue
as
you
talk,
or
your
fingertips
as
you
tap
out
posts
is
one
of
the
biggest
technical
challenges
facing
artificial
intelligence
researchers.
But
it
is
an
essential
need.
Automatic
text
processing
forms
a
key
part
of
the
day-to-day
interaction
with
your
computer;
it’s
a
critical
component
of
everything
from
web
search
and
content
ranking
to
spam
filtering,
and
when
it
works
well,
it’s
completely
invisible
to
you.
With
the
growing
amount
of
online
data,
there
is
a
need
for
more
flexible
tools
to
better
understand
the
content
of
very
large
datasets,
in
order
to
provide
more
accurate
classification
results.
</s>
</s>
To
address
this
need,
the
Facebook
AI
Research
(FAIR)
lab
is
open-sourcing
fastText,
a
library
designed
to
help
build
scalable
solutions
for
text
representation
and
classification.
Our
ongoing
commitment
to
collaboration
and
sharing
with
the
community
extends
beyond
just
delivering
code.
We
know
it’s
important
to
share
our
learnings
to
advance
the
field,
so
have
also
published
our
research
relating
to
fastText.
</s>
</s>
FastText
combines
some
of
the
most
successful
concepts
introduced
by
the
natural
language
processing
and
machine
learning
communities
in
the
last
few
decades.
These
include
representing
sentences
with
bag
of
words
and
bag
of
n-grams,
as
well
as
using
subword
information,
and
sharing
information
across
classes
through
a
hidden
representation.
We
also
employ
a
hierachical
softmax
that
takes
advantage
of
the
unbalanced
distribution
of
the
classes
to
speed
up
computation.
These
different
concepts
are
being
used
for
two
different
tasks:
efficient
text
classification
and
learning
word
vector
representations.
</s>
Efficient
learning
for
text
classification
#
</s>
</s>
Deep
neural
networks
have
recently
become
very
popular
for
text
processing.
While
these
models
achieve
very
good
performance
in
limited
laboratory
practice,
they
can
be
slow
to
train
and
test,
which
limits
their
use
on
very
large
datasets.
</s>
</s>
FastText
helps
solve
this
problem.
To
be
efficient
on
datasets
with
very
large
number
of
categories,
it
uses
a
hierarchical
classifier
instead
of
a
flat
structure,
in
which
the
different
categories
are
organized
in
a
tree
(think
binary
tree
instead
of
list).
This
reduces
the
time
complexities
of
training
and
testing
text
classifiers
from
linear
to
logarithmic
with
respect
to
the
number
of
classes.
FastText
also
exploits
the
fact
that
classes
are
imbalanced
(some
classes
appearing
more
often
than
other)
by
using
the
Huffman
algorithm
to
build
the
tree
used
to
represent
categories.
The
depth
in
the
tree
of
very
frequent
categories
is
therefore
smaller
than
for
infrequent
ones,
leading
to
further
computational
efficiency.
</s>
</s>
FastText
also
represents
a
text
by
a
low
dimensional
vector,
which
is
obtained
by
summing
vectors
corresponding
to
the
words
appearing
in
the
text.
In
fastText,
a
low
dimensional
vector
is
associated
to
each
word
of
the
vocabulary.
This
hidden
representation
is
shared
across
all
classifiers
for
different
categories,
allowing
information
about
words
learned
for
one
category
to
be
used
by
other
categories.
These
kind
of
representations,
called
bag
of
words,
ignore
word
order.
In
fastText
we
also
use
vectors
to
represent
word
ngrams
to
take
into
account
local
word
order,
which
is
important
for
many
text
classification
problems.
</s>
</s>
Our
experiments
show
that
fastText
is
often
on
par
with
deep
learning
classifiers
in
terms
of
accuracy,
and
many
orders
of
magnitude
faster
for
training
and
evaluation.
With
fastText,
we
were
often
able
to
cut
training
times
from
several
days
to
just
a
few
seconds,
and
achieve
state-of-the-art
performance
on
many
standard
problems,
such
as
sentiment
analysis
or
tag
prediction.
</s>
</s>
fastText
performance
Comparison
between
fastText
and
deep
learning-based
methods.
</s>
A
dedicated
tool
#
</s>
</s>
Text
classification
is
very
important
in
the
commercial
world;
spam
or
clickbait
filtering
being
perhaps
the
most
ubiquitous
example.
There
are
tools
that
design
models
for
general
classification
problems
(such
as
Vowpal
Wabbit
or
libSVM),
but
fastText
is
exclusively
dedicated
to
text
classification.
This
allows
it
to
be
quickly
trained
on
extremely
large
datasets.
We
have
seen
results
of
models
trained
on
more
than
1
billion
words
in
less
than
10
minutes
using
a
standard
multicore
CPU.
FastText
can
also
classify
a
half-million
sentences
among
more
than
300,000
categories
in
less
than
five
minutes.
</s>
Works
on
many
languages
#
</s>
</s>
Besides
text
classification,
fastText
can
also
be
used
to
learn
vector
representations
of
words.
It
has
been
designed
to
work
on
a
variety
of
languages,
including
English,
German,
Spanish,
French,
and
Czech,
by
taking
advantage
of
the
languages
morphological
structure.
It
uses
a
simple
yet
effective
way
of
incorporating
subword
information
that
turns
out
to
work
very
well
for
morphologically
rich
languages
like
Czech,
demonstrating
that
carefully
designed
character
ngram
features
are
strong
source
of
information
to
enrich
the
word
representations.
FastText
can
achieve
significantly
better
performance
than
the
popular
word2vec
tool,
or
other
state-of-the-art
morphological
word
representations.
</s>
</s>
fastText
performance
Comparison
between
fastText
and
state-of-the-art
word
representations
for
different
languages.
</s>
</s>
We
hope
the
introduction
of
fastText
helps
the
community
build
better,
more
scalable
solutions
for
text
representation
and
classification.
Delivered
as
an
open-source
library,
we
believe
fastText
is
a
valuable
addition
to
the
research
and
engineering
communities,
which
will
ultimately
help
us
all
design
better
applications
and
further
advances
in
language
understanding.
</s>
</s>
</s>
Skip
to
content
</s>
</s>
Features
</s>
Business
</s>
Explore
</s>
Marketplace
</s>
Pricing
</s>
</s>
This
repository
</s>
Sign
in
or
Sign
up
</s>
</s>
672
</s>
10,879
</s>
</s>
1,729
</s>
</s>
facebookresearch/fastText
</s>
Code
</s>
Issues
56
</s>
Pull
requests
53
</s>
Projects
0
</s>
Insights
</s>
Join
GitHub
today
</s>
</s>
GitHub
is
home
to
over
20
million
developers
working
together
to
host
and
review
code,
manage
projects,
and
build
software
together.
</s>
</s>
Library
for
fast
text
representation
and
classification.
</s>
</s>
201
commits
</s>
1
branch
</s>
0
releases
</s>
18
contributors
</s>
BSD-3-Clause
</s>
</s>
C++
83.4%
Shell
11.4%
Python
2.0%
Perl
1.8%
Makefile
1.4%
</s>
</s>
Latest
commit
431c9e2
5
days
ago
@EdouardGrave
EdouardGrave
committed
with
facebook-github-bot
Print
error
message
for
invalid
pretrained
model
files
</s>
src
Print
error
message
for
invalid
pretrained
model
files
5
days
ago
</s>
tutorials
minor
edits
on
tutorials
6
months
ago
</s>
.gitignore
corrected
gitignore
a
year
ago
</s>
CONTRIBUTING.md
Updated
CONTRIBUTING.md
a
year
ago
</s>
LICENSE
initial
commit
a
year
ago
</s>
Makefile
Quantization
6
months
ago
</s>
PATENTS
initial
commit
a
year
ago
</s>
README.md
add
link
to
website/faq/cheatsheet
a
month
ago
</s>
classification-example.sh
fix
link
in
bash
script
a
year
ago
</s>
classification-results.sh
Fix
google
drive
links
in
classification-results.sh
-
Issue
193
6
months
ago
</s>
eval.py
script
to
evaluate
trained
models
a
month
ago
</s>
get-wikimedia.sh
build
wiki
data
script
4
months
ago
</s>
pretrained-vectors.md
Add
links
to
pretrained
word
vectors
for
294
languages
6
months
ago
</s>
quantization-example.sh
update
the
readme
6
months
ago
</s>
quantization-results.sh
create
grid
of
models
w/
and
w/o
quantization
a
month
ago
</s>
wikifil.pl
Corrected
brace
escaping
in
perl
script
8
months
ago
</s>
word-vector-example.sh
update
rw.zip
link
again
5
months
ago
</s>
README.md
</s>
fastText
</s>
</s>
fastText
is
a
library
for
efficient
learning
of
word
representations
and
sentence
classification.
</s>
FAQ
/
Cheatsheet
</s>
</s>
You
can
find
answers
to
frequently
asked
questions
on
our
website.
</s>
</s>
We
also
provide
a
cheatsheet
full
of
useful
one-liners.
</s>
Requirements
</s>
</s>
fastText
builds
on
modern
Mac
OS
and
Linux
distributions.
Since
it
uses
C++11
features,
it
requires
a
compiler
with
good
C++11
support.
These
include
:
</s>
</s>
(gcc-4.6.3
or
newer)
or
(clang-3.3
or
newer)
</s>
</s>
Compilation
is
carried
out
using
a
Makefile,
so
you
will
need
to
have
a
working
make.
For
the
word-similarity
evaluation
script
you
will
need:
</s>
</s>
python
2.6
or
newer
</s>
numpy
&
scipy
</s>
</s>
Building
fastText
</s>
</s>
In
order
to
build
fastText,
use
the
following:
</s>
</s>
$
git
clone
https://github.com/facebookresearch/fastText.git
</s>
$
cd
fastText
</s>
$
make
</s>
</s>
This
will
produce
object
files
for
all
the
classes
as
well
as
the
main
binary
fasttext.
If
you
do
not
plan
on
using
the
default
system-wide
compiler,
update
the
two
macros
defined
at
the
beginning
of
the
Makefile
(CC
and
INCLUDES).
</s>
Example
use
cases
</s>
</s>
This
library
has
two
main
use
cases:
word
representation
learning
and
text
classification.
These
were
described
in
the
two
papers
1
and
2.
</s>
Word
representation
learning
</s>
</s>
In
order
to
learn
word
vectors,
as
described
in
1,
do:
</s>
</s>
$
./fasttext
skipgram
-input
data.txt
-output
model
</s>
</s>
where
data.txt
is
a
training
file
containing
utf-8
encoded
text.
By
default
the
word
vectors
will
take
into
account
character
n-grams
from
3
to
6
characters.
At
the
end
of
optimization
the
program
will
save
two
files:
model.bin
and
model.vec.
model.vec
is
a
text
file
containing
the
word
vectors,
one
per
line.
model.bin
is
a
binary
file
containing
the
parameters
of
the
model
along
with
the
dictionary
and
all
hyper
parameters.
The
binary
file
can
be
used
later
to
compute
word
vectors
or
to
restart
the
optimization.
</s>
Obtaining
word
vectors
for
out-of-vocabulary
words
</s>
</s>
The
previously
trained
model
can
be
used
to
compute
word
vectors
for
out-of-vocabulary
words.
Provided
you
have
a
text
file
queries.txt
containing
words
for
which
you
want
to
compute
vectors,
use
the
following
command:
</s>
</s>
$
./fasttext
print-word-vectors
model.bin
<
queries.txt
</s>
</s>
This
will
output
word
vectors
to
the
standard
output,
one
vector
per
line.
This
can
also
be
used
with
pipes:
</s>
</s>
$
cat
queries.txt
|
./fasttext
print-word-vectors
model.bin
</s>
</s>
See
the
provided
scripts
for
an
example.
For
instance,
running:
</s>
</s>
$
./word-vector-example.sh
</s>
</s>
will
compile
the
code,
download
data,
compute
word
vectors
and
evaluate
them
on
the
rare
words
similarity
dataset
RW
[Thang
et
al.
2013].
</s>
Text
classification
</s>
</s>
This
library
can
also
be
used
to
train
supervised
text
classifiers,
for
instance
for
sentiment
analysis.
In
order
to
train
a
text
classifier
using
the
method
described
in
2,
use:
</s>
</s>
$
./fasttext
supervised
-input
train.txt
-output
model
</s>
</s>
where
train.txt
is
a
text
file
containing
a
training
sentence
per
line
along
with
the
labels.
By
default,
we
assume
that
labels
are
words
that
are
prefixed
by
the
string
__label__.
This
will
output
two
files:
model.bin
and
model.vec.
Once
the
model
was
trained,
you
can
evaluate
it
by
computing
the
precision
and
recall
at
k
(P@k
and
R@k)
on
a
test
set
using:
</s>
</s>
$
./fasttext
test
model.bin
test.txt
k
</s>
</s>
The
argument
k
is
optional,
and
is
equal
to
1
by
default.
</s>
</s>
In
order
to
obtain
the
k
most
likely
labels
for
a
piece
of
text,
use:
</s>
</s>
$
./fasttext
predict
model.bin
test.txt
k
</s>
</s>
where
test.txt
contains
a
piece
of
text
to
classify
per
line.
Doing
so
will
print
to
the
standard
output
the
k
most
likely
labels
for
each
line.
The
argument
k
is
optional,
and
equal
to
1
by
default.
See
classification-example.sh
for
an
example
use
case.
In
order
to
reproduce
results
from
the
paper
2,
run
classification-results.sh,
this
will
download
all
the
datasets
and
reproduce
the
results
from
Table
1.
</s>
</s>
If
you
want
to
compute
vector
representations
of
sentences
or
paragraphs,
please
use:
</s>
</s>
$
./fasttext
print-sentence-vectors
model.bin
<
text.txt
</s>
</s>
This
assumes
that
the
text.txt
file
contains
the
paragraphs
that
you
want
to
get
vectors
for.
The
program
will
output
one
vector
representation
per
line
in
the
file.
</s>
</s>
You
can
also
quantize
a
supervised
model
to
reduce
its
memory
usage
with
the
following
command:
</s>
</s>
$
./fasttext
quantize
-output
model
</s>
</s>
This
will
create
a
.ftz
file
with
a
smaller
memory
footprint.
All
the
standard
functionality,
like
test
or
predict
work
the
same
way
on
the
quantized
models:
</s>
</s>
$
./fasttext
test
model.ftz
test.txt
</s>
</s>
The
quantization
procedure
follows
the
steps
described
in
3.
You
can
run
the
script
quantization-example.sh
for
an
example.
</s>
Full
documentation
</s>
</s>
Invoke
a
command
without
arguments
to
list
available
arguments
and
their
default
values:
</s>
</s>
$
./fasttext
supervised
</s>
Empty
input
or
output
path.
</s>
</s>
The
following
arguments
are
mandatory:
</s>
-input
training
file
path
</s>
-output
output
file
path
</s>
</s>
The
following
arguments
are
optional:
</s>
-verbose
verbosity
level
[2]
</s>
</s>
The
following
arguments
for
the
dictionary
are
optional:
</s>
-minCount
minimal
number
of
word
occurences
[5]
</s>
-minCountLabel
minimal
number
of
label
occurences
[0]
</s>
-wordNgrams
max
length
of
word
ngram
[1]
</s>
-bucket
number
of
buckets
[2000000]
</s>
-minn
min
length
of
char
ngram
[3]
</s>
-maxn
max
length
of
char
ngram
[6]
</s>
-t
sampling
threshold
[0.0001]
</s>
-label
labels
prefix
[__label__]
</s>
</s>
The
following
arguments
for
training
are
optional:
</s>
-lr
learning
rate
[0.05]
</s>
-lrUpdateRate
change
the
rate
of
updates
for
the
learning
rate
[100]
</s>
-dim
size
of
word
vectors
[100]
</s>
-ws
size
of
the
context
window
[5]
</s>
-epoch
number
of
epochs
[5]
</s>
-neg
number
of
negatives
sampled
[5]
</s>
-loss
loss
function
{ns,
hs,
softmax}
[ns]
</s>
-thread
number
of
threads
[12]
</s>
-pretrainedVectors
pretrained
word
vectors
for
supervised
learning
[]
</s>
-saveOutput
whether
output
params
should
be
saved
[0]
</s>
</s>
The
following
arguments
for
quantization
are
optional:
</s>
-cutoff
number
of
words
and
ngrams
to
retain
[0]
</s>
-retrain
finetune
embeddings
if
a
cutoff
is
applied
[0]
</s>
-qnorm
quantizing
the
norm
separately
[0]
</s>
-qout
quantizing
the
classifier
[0]
</s>
-dsub
size
of
each
sub-vector
[2]
</s>
</s>
Defaults
may
vary
by
mode.
(Word-representation
modes
skipgram
and
cbow
use
a
default
-minCount
of
5.)
</s>
References
</s>
</s>
Please
cite
1
if
using
this
code
for
learning
word
representations
or
2
if
using
for
text
classification.
</s>
Enriching
Word
Vectors
with
Subword
Information
</s>
</s>
[1]
P.
Bojanowski*,
E.
Grave*,
A.
Joulin,
T.
Mikolov,
Enriching
Word
Vectors
with
Subword
Information
</s>
</s>
@article{bojanowski2016enriching,
</s>
title={Enriching
Word
Vectors
with
Subword
Information},
</s>
author={Bojanowski,
Piotr
and
Grave,
Edouard
and
Joulin,
Armand
and
Mikolov,
Tomas},
</s>
journal={arXiv
preprint
arXiv:1607.04606},
</s>
year={2016}
</s>
}
</s>
</s>
Bag
of
Tricks
for
Efficient
Text
Classification
</s>
</s>
[2]
A.
Joulin,
E.
Grave,
P.
Bojanowski,
T.
Mikolov,
Bag
of
Tricks
for
Efficient
Text
Classification
</s>
</s>
@article{joulin2016bag,
</s>
title={Bag
of
Tricks
for
Efficient
Text
Classification},
</s>
author={Joulin,
Armand
and
Grave,
Edouard
and
Bojanowski,
Piotr
and
Mikolov,
Tomas},
</s>
journal={arXiv
preprint
arXiv:1607.01759},
</s>
year={2016}
</s>
}
</s>
</s>
FastText.zip:
Compressing
text
classification
models
</s>
</s>
[3]
A.
Joulin,
E.
Grave,
P.
Bojanowski,
M.
Douze,
H.
Jégou,
T.
Mikolov,
FastText.zip:
Compressing
text
classification
models
</s>
</s>
@article{joulin2016fasttext,
</s>
title={FastText.zip:
Compressing
text
classification
models},
</s>
author={Joulin,
Armand
and
Grave,
Edouard
and
Bojanowski,
Piotr
and
Douze,
Matthijs
and
J{\'e}gou,
H{\'e}rve
and
Mikolov,
Tomas},
</s>
journal={arXiv
preprint
arXiv:1612.03651},
</s>
year={2016}
</s>
}
</s>
</s>
(*
These
authors
contributed
equally.)
</s>
Resources
</s>
</s>
You
can
find
the
preprocessed
YFCC100M
data
used
in
[2]
at
https://research.facebook.com/research/fasttext/
</s>
</s>
Pre-trained
word
vectors
for
294
languages
are
available
here.
</s>
Join
the
fastText
community
</s>
</s>
Facebook
page:
https://www.facebook.com/groups/1174547215919768
</s>
Google
group:
https://groups.google.com/forum/#!forum/fasttext-library
</s>
Contact:
egrave@fb.com,
bojanowski@fb.com,
ajoulin@fb.com,
tmikolov@fb.com
</s>
</s>
See
the
CONTRIBUTING
file
for
information
about
how
to
help
out.
</s>
License
</s>
</s>
fastText
is
BSD-licensed.
We
also
provide
an
additional
patent
grant.
</s>
</s>
©
2017
GitHub,
Inc.
</s>
Terms
</s>
Privacy
</s>
Security
</s>
Status
</s>
Help
</s>
</s>
Contact
GitHub
</s>
API
</s>
Training
</s>
Shop
</s>
Blog
</s>
About
</s>
</s>
